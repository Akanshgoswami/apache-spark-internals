= [[PairRDDFunctions]] PairRDDFunctions

`PairRDDFunctions` is an extension of `RDDs` of (key, value) pairs (`RDD[(K, V)]`) with extra <<transformations, transformations>>.

`PairRDDFunctions` is available in RDDs of key-value pairs via Scala implicit conversion.

[[transformations]]
.PairRDDFunctions' Transformations
[cols="30m,70",options="header",width="100%"]
|===
| Method
| Description

| aggregateByKey
a| [[aggregateByKey]]

[source, scala]
----
aggregateByKey[U: ClassTag](
  zeroValue: U)(
    seqOp: (U, V) => U,
    combOp: (U, U) => U): RDD[(K, U)]
aggregateByKey[U: ClassTag](
  zeroValue: U, numPartitions: Int)(
    seqOp: (U, V) => U,
    combOp: (U, U) => U): RDD[(K, U)]
aggregateByKey[U: ClassTag](
  zeroValue: U, partitioner: Partitioner)(
    seqOp: (U, V) => U,
    combOp: (U, U) => U): RDD[(K, U)]
----

| combineByKey
a| [[combineByKey]]

[source, scala]
----
combineByKey[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C): RDD[(K, C)]
combineByKey[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C,
  numPartitions: Int): RDD[(K, C)]
combineByKey[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C,
  partitioner: Partitioner,
  mapSideCombine: Boolean = true,
  serializer: Serializer = null): RDD[(K, C)]
----

| countApproxDistinctByKey
a| [[countApproxDistinctByKey]]

[source, scala]
----
countApproxDistinctByKey(
  relativeSD: Double = 0.05): RDD[(K, Long)]
countApproxDistinctByKey(
  relativeSD: Double,
  numPartitions: Int): RDD[(K, Long)]
countApproxDistinctByKey(
  relativeSD: Double,
  partitioner: Partitioner): RDD[(K, Long)]
countApproxDistinctByKey(
  p: Int,
  sp: Int,
  partitioner: Partitioner): RDD[(K, Long)]
----

| flatMapValues
a| [[flatMapValues]]

[source, scala]
----
flatMapValues[U](
  f: V => TraversableOnce[U]): RDD[(K, U)]
----

| foldByKey
a| [[foldByKey]]

[source, scala]
----
foldByKey(
  zeroValue: V)(
    func: (V, V) => V): RDD[(K, V)]
foldByKey(
  zeroValue: V, numPartitions: Int)(
    func: (V, V) => V): RDD[(K, V)]
foldByKey(
  zeroValue: V,
  partitioner: Partitioner)(
    func: (V, V) => V): RDD[(K, V)]
----

| mapValues
a| [[mapValues]]

[source, scala]
----
mapValues[U](
  f: V => U): RDD[(K, U)]
----

| partitionBy
a| [[partitionBy]]

[source, scala]
----
partitionBy(
  partitioner: Partitioner): RDD[(K, V)]
----

| saveAsHadoopDataset
a| [[saveAsHadoopDataset]]

[source, scala]
----
saveAsHadoopDataset(
  conf: JobConf): Unit
----

`saveAsHadoopDataset` uses the `SparkHadoopWriter` utility to <<spark-internal-io-SparkHadoopWriter.adoc#write, write the key-value RDD out>> with a <<spark-internal-io-HadoopMapRedWriteConfigUtil.adoc#, HadoopMapRedWriteConfigUtil>> (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapred/JobConf.html[JobConf])

| saveAsHadoopFile
a| [[saveAsHadoopFile]]

[source, scala]
----
saveAsHadoopFile(
  path: String,
  keyClass: Class[_],
  valueClass: Class[_],
  outputFormatClass: Class[_ <: OutputFormat[_, _]],
  codec: Class[_ <: CompressionCodec]): Unit
saveAsHadoopFile(
  path: String,
  keyClass: Class[_],
  valueClass: Class[_],
  outputFormatClass: Class[_ <: OutputFormat[_, _]],
  conf: JobConf = new JobConf(self.context.hadoopConfiguration),
  codec: Option[Class[_ <: CompressionCodec]] = None): Unit
saveAsHadoopFile[F <: OutputFormat[K, V]](
  path: String)(implicit fm: ClassTag[F]): Unit
saveAsHadoopFile[F <: OutputFormat[K, V]](
  path: String,
  codec: Class[_ <: CompressionCodec])(implicit fm: ClassTag[F]): Unit
----

| saveAsNewAPIHadoopDataset
a| [[saveAsNewAPIHadoopDataset]]

[source, scala]
----
saveAsNewAPIHadoopDataset(
  conf: Configuration): Unit
----

Saves this RDD of key-value pairs (`RDD[K,V]`) to any Hadoop-supported storage system with new Hadoop API (using a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration] object for that storage system).

The configuration should set relevant output params (an https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/OutputFormat.html[output format], output paths, e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job.

`saveAsNewAPIHadoopDataset` uses the `SparkHadoopWriter` utility to <<spark-internal-io-SparkHadoopWriter.adoc#write, write the key-value RDD out>> with a <<spark-internal-io-HadoopMapReduceWriteConfigUtil.adoc#, HadoopMapReduceWriteConfigUtil>> (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration])

| saveAsNewAPIHadoopFile
a| [[saveAsNewAPIHadoopFile]]

[source, scala]
----
saveAsNewAPIHadoopFile(
  path: String,
  keyClass: Class[_],
  valueClass: Class[_],
  outputFormatClass: Class[_ <: NewOutputFormat[_, _]],
  conf: Configuration = self.context.hadoopConfiguration): Unit
saveAsNewAPIHadoopFile[F <: NewOutputFormat[K, V]](
  path: String)(implicit fm: ClassTag[F]): Unit
----

|===

== [[reduceByKey]][[groupByKey]] `groupByKey` and `reduceByKey` Transformations

`reduceByKey` is sort of a particular case of <<aggregateByKey, aggregateByKey>>.

You may want to look at the number of partitions from another angle.

It may often not be important to have a given number of partitions upfront (at RDD creation time upon link:spark-data-sources.adoc[loading data from data sources]), so only "regrouping" the data by key after it is an RDD might be...the key (_pun not intended_).

You can use `groupByKey` or another `PairRDDFunctions` method to have a key in one processing flow.

You could use `partitionBy` that is available for RDDs to be RDDs of tuples, i.e. `PairRDD`:

```
rdd.keyBy(_.kind)
  .partitionBy(new HashPartitioner(PARTITIONS))
  .foreachPartition(...)
```

Think of situations where `kind` has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution.

You could do as follows:

```
rdd.keyBy(_.kind).reduceByKey(....)
```

or `mapValues` or plenty of other solutions. _FIXME, man_.

== [[combineByKeyWithClassTag]] `combineByKeyWithClassTag` Transformations

[source, scala]
----
combineByKeyWithClassTag[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C)(implicit ct: ClassTag[C]): RDD[(K, C)] // <1>
combineByKeyWithClassTag[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C,
  numPartitions: Int)(implicit ct: ClassTag[C]): RDD[(K, C)] // <2>
combineByKeyWithClassTag[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C,
  partitioner: Partitioner,
  mapSideCombine: Boolean = true,
  serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)]
----
<1> FIXME
<2> FIXME too

`combineByKeyWithClassTag` transformations use `mapSideCombine` enabled (i.e. `true`) by default. They create a link:spark-rdd-ShuffledRDD.adoc[ShuffledRDD] with the value of `mapSideCombine` when the input partitioner is different from the current one in an RDD.

NOTE: `combineByKeyWithClassTag` is a base transformation for <<combineByKey, combineByKey>>-based transformations, <<aggregateByKey, aggregateByKey>>, <<foldByKey, foldByKey>>, <<reduceByKey, reduceByKey>>, <<countApproxDistinctByKey, countApproxDistinctByKey>>, and <<groupByKey, groupByKey>>.
