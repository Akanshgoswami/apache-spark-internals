= [[ShuffleDependency]] ShuffleDependency

*ShuffleDependency* is a xref:rdd:spark-rdd-Dependency.adoc[Dependency] on the output of a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] for a <<rdd, key-value pair RDD>>.

ShuffleDependency uses the <<rdd, RDD>> to know the number of (map-side/pre-shuffle) partitions and the <<partitioner, Partitioner>> for the number of (reduce-size/post-shuffle) partitions.

ShuffleDependency is created as a dependency of xref:rdd:ShuffledRDD.adoc[ShuffledRDD]. ShuffleDependency can also be created as a dependency of xref:rdd:spark-rdd-CoGroupedRDD.adoc[CoGroupedRDD] and xref:rdd:spark-rdd-SubtractedRDD.adoc[SubtractedRDD].

== [[creating-instance]] Creating Instance

ShuffleDependency takes the following to be created:

* <<rdd, RDD>> of key-value pairs (`RDD[_ <: Product2[K, V]]`)
* <<partitioner, Partitioner>>
* [[serializer]] xref:ROOT:spark-Serializer.adoc[Serializer]
* [[keyOrdering]] Ordering for K keys (`Option[Ordering[K]]`)
* <<aggregator, Aggregator>> (`Option[Aggregator[K, V, C]]`)
* <<mapSideCombine, mapSideCombine>> flag (default: `false`)

When created, ShuffleDependency gets link:spark-SparkContext.adoc#nextShuffleId[shuffle id] (as `shuffleId`).

NOTE: ShuffleDependency uses the xref:rdd:index.adoc#context[input RDD to access `SparkContext`] and so the `shuffleId`.

ShuffleDependency xref:shuffle:ShuffleManager.adoc#registerShuffle[registers itself with `ShuffleManager`] and gets a `ShuffleHandle` (available as <<shuffleHandle, shuffleHandle>> property).

NOTE: ShuffleDependency accesses link:spark-SparkEnv.adoc#shuffleManager[`ShuffleManager` using `SparkEnv`].

In the end, ShuffleDependency link:spark-service-contextcleaner.adoc#registerShuffleForCleanup[registers itself for cleanup with `ContextCleaner`].

NOTE: ShuffleDependency accesses the link:spark-SparkContext.adoc#cleaner[optional `ContextCleaner` through `SparkContext`].

NOTE: ShuffleDependency is created when xref:ShuffledRDD.adoc#getDependencies[ShuffledRDD], link:spark-rdd-CoGroupedRDD.adoc#getDependencies[CoGroupedRDD], and link:spark-rdd-SubtractedRDD.adoc#getDependencies[SubtractedRDD] return their RDD dependencies.

== [[shuffleId]] Shuffle ID

Every ShuffleDependency has a unique application-wide *shuffle ID* that is assigned when <<creating-instance, ShuffleDependency is created>> (and is used throughout Spark's code to reference a ShuffleDependency).

Shuffle IDs are tracked by xref:ROOT:spark-SparkContext.adoc#nextShuffleId[SparkContext].

== [[rdd]] Parent RDD

ShuffleDependency is given the parent xref:rdd:spark-rdd-RDD.adoc[RDD] of key-value pairs (`RDD[_ <: Product2[K, V]]`).

The parent RDD is available as rdd property that is part of the xref:rdd:spark-rdd-Dependency.adoc#rdd[Dependency] abstraction.

[source,scala]
----
rdd: RDD[Product2[K, V]]
----

== [[partitioner]] Partitioner

ShuffleDependency is given a xref:rdd:spark-rdd-Partitioner.adoc[Partitioner] that is used to partition the shuffle output (when xref:shuffle:spark-shuffle-SortShuffleWriter.adoc[SortShuffleWriter], xref:shuffle:spark-shuffle-BypassMergeSortShuffleWriter.adoc[BypassMergeSortShuffleWriter] and xref:shuffle:spark-shuffle-UnsafeShuffleWriter.adoc[UnsafeShuffleWriter] are requested to write).

== [[shuffleHandle]] ShuffleHandle

[source, scala]
----
shuffleHandle: ShuffleHandle
----

`shuffleHandle` is the `ShuffleHandle` of a ShuffleDependency as assigned eagerly when <<creating-instance, ShuffleDependency was created>>.

NOTE: `shuffleHandle` is used to compute link:spark-rdd-CoGroupedRDD.adoc#compute[CoGroupedRDDs], xref:ShuffledRDD.adoc#compute[ShuffledRDD], link:spark-rdd-SubtractedRDD.adoc#compute[SubtractedRDD], and link:spark-sql-ShuffledRowRDD.adoc[ShuffledRowRDD] (to get a link:spark-shuffle-ShuffleReader.adoc[ShuffleReader] for a ShuffleDependency) and when a xref:scheduler:ShuffleMapTask.adoc#runTask[`ShuffleMapTask` runs] (to get a `ShuffleWriter` for a ShuffleDependency).

== [[mapSideCombine]] Map-Size Aggregation Flag

`mapSideCombine` is a flag to control whether to use *partial aggregation* (_map-side combine_).

`mapSideCombine` is by default disabled (i.e. `false`) when <<creating-instance, creating a ShuffleDependency>>.

When enabled, xref:shuffle:spark-shuffle-SortShuffleWriter.adoc[SortShuffleWriter] and link:spark-shuffle-BlockStoreShuffleReader.adoc[BlockStoreShuffleReader] assume that an link:spark-Aggregator.adoc[Aggregator] is also defined.

NOTE: `mapSideCombine` is exclusively set (and hence can be enabled) when `ShuffledRDD` is requested for xref:ShuffledRDD.adoc#getDependencies[dependencies] (which is a single ShuffleDependency).

== [[aggregator]] Optional Aggregator

[source, scala]
----
aggregator: Option[Aggregator[K, V, C]] = None
----

`aggregator` is a link:spark-Aggregator.adoc[map/reduce-side Aggregator] (for a RDD's shuffle).

`aggregator` is by default undefined (i.e. `None`) when <<creating-instance, ShuffleDependency is created>>.

NOTE: `aggregator` is used when xref:shuffle:spark-shuffle-SortShuffleWriter.adoc#write[`SortShuffleWriter` writes records] and xref:shuffle:spark-shuffle-BlockStoreShuffleReader.adoc#read[`BlockStoreShuffleReader` reads combined key-values for a reduce task].
