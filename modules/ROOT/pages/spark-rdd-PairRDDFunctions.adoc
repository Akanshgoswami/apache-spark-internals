= [[PairRDDFunctions]] PairRDDFunctions

`PairRDDFunctions` is an extension of `RDDs` of (key, value) pairs (`RDD[(K, V)]`) with extra <<transformations, transformations>>.

`PairRDDFunctions` is available in RDDs of key-value pairs via Scala implicit conversion.

[[transformations]]
.PairRDDFunctions' Transformations
[cols="30m,70",options="header",width="100%"]
|===
| Method
| Description

| aggregateByKey
a| [[aggregateByKey]]

[source, scala]
----
aggregateByKey[U: ClassTag](
  zeroValue: U)(
    seqOp: (U, V) => U,
    combOp: (U, U) => U): RDD[(K, U)]
aggregateByKey[U: ClassTag](
  zeroValue: U, numPartitions: Int)(
    seqOp: (U, V) => U,
    combOp: (U, U) => U): RDD[(K, U)]
aggregateByKey[U: ClassTag](
  zeroValue: U, partitioner: Partitioner)(
    seqOp: (U, V) => U,
    combOp: (U, U) => U): RDD[(K, U)]
----

| combineByKey
a| [[combineByKey]]

[source, scala]
----
combineByKey[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C): RDD[(K, C)]
combineByKey[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C,
  numPartitions: Int): RDD[(K, C)]
combineByKey[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C,
  partitioner: Partitioner,
  mapSideCombine: Boolean = true,
  serializer: Serializer = null): RDD[(K, C)]
----

| countApproxDistinctByKey
a| [[countApproxDistinctByKey]]

[source, scala]
----
countApproxDistinctByKey(
  relativeSD: Double = 0.05): RDD[(K, Long)]
countApproxDistinctByKey(
  relativeSD: Double,
  numPartitions: Int): RDD[(K, Long)]
countApproxDistinctByKey(
  relativeSD: Double,
  partitioner: Partitioner): RDD[(K, Long)]
countApproxDistinctByKey(
  p: Int,
  sp: Int,
  partitioner: Partitioner): RDD[(K, Long)]
----

| flatMapValues
a| [[flatMapValues]]

[source, scala]
----
flatMapValues[U](
  f: V => TraversableOnce[U]): RDD[(K, U)]
----

| foldByKey
a| [[foldByKey]]

[source, scala]
----
foldByKey(
  zeroValue: V)(
    func: (V, V) => V): RDD[(K, V)]
foldByKey(
  zeroValue: V, numPartitions: Int)(
    func: (V, V) => V): RDD[(K, V)]
foldByKey(
  zeroValue: V,
  partitioner: Partitioner)(
    func: (V, V) => V): RDD[(K, V)]
----

| mapValues
a| [[mapValues]]

[source, scala]
----
mapValues[U](
  f: V => U): RDD[(K, U)]
----

| partitionBy
a| [[partitionBy]]

[source, scala]
----
partitionBy(
  partitioner: Partitioner): RDD[(K, V)]
----

| saveAsHadoopDataset
a| [[saveAsHadoopDataset]]

[source, scala]
----
saveAsHadoopDataset(
  conf: JobConf): Unit
----

`saveAsHadoopDataset` uses the `SparkHadoopWriter` utility to <<spark-internal-io-SparkHadoopWriter.adoc#write, write the key-value RDD out>> with a <<spark-internal-io-HadoopMapRedWriteConfigUtil.adoc#, HadoopMapRedWriteConfigUtil>> (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapred/JobConf.html[JobConf])

| saveAsNewAPIHadoopDataset
a| [[saveAsNewAPIHadoopDataset]]

[source, scala]
----
saveAsNewAPIHadoopDataset(
  conf: Configuration): Unit
----

`saveAsNewAPIHadoopDataset` uses the `SparkHadoopWriter` utility to <<spark-internal-io-SparkHadoopWriter.adoc#write, write the key-value RDD out>> with a <<spark-internal-io-HadoopMapReduceWriteConfigUtil.adoc#, HadoopMapReduceWriteConfigUtil>> (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration])

|===

== [[reduceByKey]][[groupByKey]] `groupByKey` and `reduceByKey` Transformations

`reduceByKey` is sort of a particular case of <<aggregateByKey, aggregateByKey>>.

You may want to look at the number of partitions from another angle.

It may often not be important to have a given number of partitions upfront (at RDD creation time upon link:spark-data-sources.adoc[loading data from data sources]), so only "regrouping" the data by key after it is an RDD might be...the key (_pun not intended_).

You can use `groupByKey` or another `PairRDDFunctions` method to have a key in one processing flow.

You could use `partitionBy` that is available for RDDs to be RDDs of tuples, i.e. `PairRDD`:

```
rdd.keyBy(_.kind)
  .partitionBy(new HashPartitioner(PARTITIONS))
  .foreachPartition(...)
```

Think of situations where `kind` has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution.

You could do as follows:

```
rdd.keyBy(_.kind).reduceByKey(....)
```

or `mapValues` or plenty of other solutions. _FIXME, man_.

== [[combineByKeyWithClassTag]] `combineByKeyWithClassTag` Transformations

[source, scala]
----
combineByKeyWithClassTag[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C)(implicit ct: ClassTag[C]): RDD[(K, C)] // <1>
combineByKeyWithClassTag[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C,
  numPartitions: Int)(implicit ct: ClassTag[C]): RDD[(K, C)] // <2>
combineByKeyWithClassTag[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C,
  partitioner: Partitioner,
  mapSideCombine: Boolean = true,
  serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)]
----
<1> FIXME
<2> FIXME too

`combineByKeyWithClassTag` transformations use `mapSideCombine` enabled (i.e. `true`) by default. They create a link:spark-rdd-ShuffledRDD.adoc[ShuffledRDD] with the value of `mapSideCombine` when the input partitioner is different from the current one in an RDD.

NOTE: `combineByKeyWithClassTag` is a base transformation for <<combineByKey, combineByKey>>-based transformations, <<aggregateByKey, aggregateByKey>>, <<foldByKey, foldByKey>>, <<reduceByKey, reduceByKey>>, <<countApproxDistinctByKey, countApproxDistinctByKey>>, and <<groupByKey, groupByKey>>.
