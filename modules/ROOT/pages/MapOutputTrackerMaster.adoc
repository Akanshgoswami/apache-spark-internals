= [[MapOutputTrackerMaster]] MapOutputTrackerMaster

*MapOutputTrackerMaster* is the xref:ROOT:MapOutputTracker.adoc[MapOutputTracker] for the driver.

A MapOutputTrackerMaster is the source of truth for xref:scheduler:spark-scheduler-MapStatus.adoc[MapStatus] objects (map output locations) per shuffle id (as recorded from xref:scheduler:ShuffleMapTask.adoc[ShuffleMapTasks]).

NOTE: There is currently a hardcoded limit of map and reduce tasks above which Spark does not assign preferred locations aka locality preferences based on map output sizes -- `1000` for map and reduce each.

MapOutputTrackerMaster uses `MetadataCleaner` with `MetadataCleanerType.MAP_OUTPUT_TRACKER` as `cleanerType` and <<cleanup, cleanup>> function to drop entries in `mapStatuses`.

== [[creating-instance]] Creating Instance

MapOutputTrackerMaster takes the following to be created:

* [[conf]] xref:ROOT:spark-SparkConf.adoc[SparkConf]
* [[broadcastManager]] xref:ROOT:spark-service-broadcastmanager.adoc[BroadcastManager]
* [[isLocal]] isLocal flag (whether MapOutputTrackerMaster runs in local or on a cluster)

MapOutputTrackerMaster initializes the <<internal-properties, internal properties>> and <<threadpool, starts map-output-dispatcher threads>>.

== [[mapOutputRequests]][[GetMapOutputMessage]] GetMapOutputMessage Queue

MapOutputTrackerMaster uses a blocking queue (a Java {java-javadoc-url}/java/util/concurrent/LinkedBlockingQueue.html[LinkedBlockingQueue]) for requests for map output statuses.

[source,scala]
----
GetMapOutputMessage(shuffleId: Int, context: RpcCallContext)
----

GetMapOutputMessage holds the shuffle ID and the RpcCallContext of the caller.

A new GetMapOutputMessage is added to the queue when MapOutputTrackerMaster is requested to <<post, post one>>.

MapOutputTrackerMaster uses <<MessageLoop, MessageLoop Dispatcher Threads>> to process GetMapOutputMessages.

== [[MessageLoop]][[run]] MessageLoop Dispatcher Thread

MessageLoop is a thread of execution to handle <<GetMapOutputMessage, GetMapOutputMessages>> until <<PoisonPill, PoisonPill>> arrives.

MessageLoop takes a GetMapOutputMessage and prints out the following DEBUG message to the logs:

[source,plaintext]
----
Handling request to send map output locations for shuffle [shuffleId] to [hostPort]
----

MessageLoop then finds the xref:ROOT:ShuffleStatus.adoc[ShuffleStatus] by the shuffle ID in the <<shuffleStatuses, shuffleStatuses>> internal registry and replies back (to the RPC client) with a xref:ROOT:ShuffleStatus.adoc#serializedMapStatus[serialized map output status] (with the <<broadcastManager, BroadcastManager>> and <<spark.shuffle.mapOutput.minSizeForBroadcast, spark.shuffle.mapOutput.minSizeForBroadcast>> configuration property).

MessageLoop threads run on the <<threadpool, map-output-dispatcher Thread Pool>>.

== [[threadpool]] map-output-dispatcher Thread Pool

[source, scala]
----
threadpool: ThreadPoolExecutor
----

threadpool is a daemon fixed thread pool registered with *map-output-dispatcher* thread name prefix.

threadpool uses xref:ROOT:spark-configuration-properties.adoc#spark.shuffle.mapOutput.dispatcher.numThreads[spark.shuffle.mapOutput.dispatcher.numThreads] configuration property for the number of <<MessageLoop, MessageLoop dispatcher threads>> to process received `GetMapOutputMessage` messages.

The dispatcher threads are started immediately when <<creating-instance, MapOutputTrackerMaster is created>>.

The thread pool is shut down when MapOutputTrackerMaster is requested to <<stop, stop>>.

== [[configuration-properties]] Configuration Properties

MapOutputTrackerMaster uses the following configuration properties:

* [[spark.shuffle.mapOutput.minSizeForBroadcast]] xref:ROOT:spark-configuration-properties.adoc#spark.shuffle.mapOutput.minSizeForBroadcast[spark.shuffle.mapOutput.minSizeForBroadcast]

* [[spark.shuffle.mapOutput.dispatcher.numThreads]] xref:ROOT:spark-configuration-properties.adoc#spark.shuffle.mapOutput.dispatcher.numThreads[spark.shuffle.mapOutput.dispatcher.numThreads]

== [[shuffleStatuses]] Shuffle Status Registry

MapOutputTrackerMaster uses an internal registry for xref:ROOT:ShuffleStatus.adoc[ShuffleStatuses] by shuffle ID.

MapOutputTrackerMaster adds a new shuffle when requested to <<registerShuffle, register one>>.

MapOutputTrackerMaster uses the registry when requested for the following:

* <<registerMapOutput, registerMapOutput>>

* <<getStatistics, getStatistics>>

* <<MessageLoop, MessageLoop>>

* <<unregisterMapOutput, unregisterMapOutput>>, <<unregisterAllMapOutput, unregisterAllMapOutput>>, <<unregisterShuffle, unregisterShuffle>>, <<removeOutputsOnHost, removeOutputsOnHost>>, <<removeOutputsOnExecutor, removeOutputsOnExecutor>>, <<containsShuffle, containsShuffle>>, <<getNumAvailableOutputs, getNumAvailableOutputs>>, <<findMissingPartitions, findMissingPartitions>>, <<getLocationsWithLargestOutputs, getLocationsWithLargestOutputs>>, <<getMapSizesByExecutorId, getMapSizesByExecutorId>>

MapOutputTrackerMaster removes (_clears_) all shuffles when requested to <<stop, stop>>.

== [[epoch]][[getEpoch]] Epoch Number

MapOutputTrackerMaster uses an *epoch number* to...FIXME

== [[post]] Enqueueing GetMapOutputMessage

[source, scala]
----
post(
  message: GetMapOutputMessage): Unit
----

post simply adds the input GetMapOutputMessage to the <<mapOutputRequests, mapOutputRequests>> internal queue.

post is used when MapOutputTrackerMasterEndpoint is requested to xref:ROOT:MapOutputTrackerMasterEndpoint.adoc#GetMapOutputStatuses[handle a GetMapOutputStatuses message].

== [[removeBroadcast]] `removeBroadcast` Method

CAUTION: FIXME

== [[clearCachedBroadcast]] `clearCachedBroadcast` Method

CAUTION: FIXME

== [[stop]] `stop` Method

CAUTION: FIXME

== [[unregisterMapOutput]] `unregisterMapOutput` Method

CAUTION: FIXME

== [[cleanup]] cleanup Function for MetadataCleaner

`cleanup(cleanupTime: Long)` method removes old entries in `mapStatuses` and `cachedSerializedStatuses` that have timestamp earlier than `cleanupTime`.

It uses `org.apache.spark.util.TimeStampedHashMap.clearOldValues` method.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.util.TimeStampedHashMap` logger to see what happens in TimeStampedHashMap.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.util.TimeStampedHashMap=DEBUG
```
====

You should see the following DEBUG message in the logs for entries being removed:

```
DEBUG Removing key [entry.getKey]
```

== [[getPreferredLocationsForShuffle]] Finding Preferred BlockManagers with Most Shuffle Map Outputs (For ShuffleDependency and Partition) -- getPreferredLocationsForShuffle Method

[source, scala]
----
getPreferredLocationsForShuffle(dep: ShuffleDependency[_, _, _], partitionId: Int): Seq[String]
----

getPreferredLocationsForShuffle finds the locations (i.e. xref:storage:BlockManager.adoc[BlockManagers]) with the most map outputs for the input xref:rdd:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and xref:rdd:spark-rdd-Partition.adoc[Partition].

NOTE: getPreferredLocationsForShuffle is simply <<getLocationsWithLargestOutputs, getLocationsWithLargestOutputs>> with a guard condition.

Internally, getPreferredLocationsForShuffle checks whether <<spark_shuffle_reduceLocality_enabled, `spark.shuffle.reduceLocality.enabled` Spark property>> is enabled (it is by default) with the number of partitions of the xref:rdd:spark-rdd-ShuffleDependency.adoc#rdd[RDD of the input `ShuffleDependency`] and partitions in the xref:rdd:spark-rdd-ShuffleDependency.adoc#partitioner[partitioner of the input `ShuffleDependency`] both being less than `1000`.

NOTE: The thresholds for the number of partitions in the RDD and of the partitioner when computing the preferred locations are `1000` and are not configurable.

If the condition holds, getPreferredLocationsForShuffle <<getLocationsWithLargestOutputs, finds locations with the largest number of shuffle map outputs>> for the input `ShuffleDependency` and `partitionId` (with the number of partitions in the partitioner of the input `ShuffleDependency` and `0.2`) and returns the hosts of the preferred `BlockManagers`.

NOTE: `0.2` is the fraction of total map output that must be at a location to be considered as a preferred location for a reduce task. It is not configurable.

getPreferredLocationsForShuffle is used when xref:rdd:ShuffledRDD.adoc#getPreferredLocations[ShuffledRDD] and Spark SQL's ShuffledRowRDD ask for preferred locations for a partition.

== [[incrementEpoch]] Incrementing Epoch -- `incrementEpoch` Method

[source, scala]
----
incrementEpoch(): Unit
----

`incrementEpoch` increments the internal xref:ROOT:MapOutputTracker.adoc#epoch[epoch].

You should see the following DEBUG message in the logs:

```
DEBUG MapOutputTrackerMaster: Increasing epoch to [epoch]
```

NOTE: `incrementEpoch` is used when MapOutputTrackerMaster <<registerMapOutputs, registers map outputs>> (with `changeEpoch` flag enabled -- it is disabled by default) and <<unregisterMapOutput, unregisters map outputs>> (for a shuffle, mapper and block manager), and when xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[`DAGScheduler` is notified that an executor got lost] (with `filesLost` flag enabled).

== [[getLocationsWithLargestOutputs]] Finding Locations with Largest Number of Shuffle Map Outputs -- getLocationsWithLargestOutputs Method

[source, scala]
----
getLocationsWithLargestOutputs(
  shuffleId: Int,
  reducerId: Int,
  numReducers: Int,
  fractionThreshold: Double): Option[Array[BlockManagerId]]
----

getLocationsWithLargestOutputs returns xref:storage:BlockManager.adoc#BlockManagerId[BlockManagerId]s with the largest size (of all the shuffle blocks they manage) above the input `fractionThreshold` (given the total size of all the shuffle blocks for the shuffle across all xref:storage:BlockManager.adoc[BlockManagers]).

NOTE: getLocationsWithLargestOutputs may return no `BlockManagerId` if their shuffle blocks do not total up above the input `fractionThreshold`.

NOTE: The input `numReducers` is not used.

Internally, getLocationsWithLargestOutputs queries the <<mapStatuses, mapStatuses>> internal cache for the input `shuffleId`.

[NOTE]
====
One entry in `mapStatuses` internal cache is a xref:scheduler:spark-scheduler-MapStatus.adoc[MapStatus] array indexed by partition id.

`MapStatus` includes xref:scheduler:spark-scheduler-MapStatus.adoc#contract[information about the `BlockManager` (as `BlockManagerId`) and estimated size of the reduce blocks].
====

getLocationsWithLargestOutputs iterates over the `MapStatus` array and builds an interim mapping between xref:storage:BlockManager.adoc#BlockManagerId[BlockManagerId] and the cumulative sum of shuffle blocks across xref:storage:BlockManager.adoc[BlockManagers].

getLocationsWithLargestOutputs is used when MapOutputTrackerMaster is requested for the <<getPreferredLocationsForShuffle, preferred locations (BlockManagers and hence executors) for a shuffle>>.

== [[containsShuffle]] Requesting Tracking Status of Shuffle Map Output

[source, scala]
----
containsShuffle(shuffleId: Int): Boolean
----

containsShuffle checks if the input `shuffleId` is registered in the <<cachedSerializedStatuses, cachedSerializedStatuses>> or xref:ROOT:MapOutputTracker.adoc#mapStatuses[mapStatuses] internal caches.

containsShuffle is used when xref:scheduler:DAGScheduler.adoc#createShuffleMapStage[`DAGScheduler` creates a `ShuffleMapStage`] (for xref:rdd:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and xref:scheduler:spark-scheduler-ActiveJob.adoc[ActiveJob]).

== [[registerShuffle]] Registering Shuffle

[source, scala]
----
registerShuffle(
  shuffleId: Int,
  numMaps: Int): Unit
----

registerShuffle adds the input shuffle ID and the number of partitions (as a xref:ROOT:ShuffleStatus.adoc[ShuffleStatus]) to <<shuffleStatuses, shuffleStatuses>> internal registry.

If the shuffle ID has already been registered, registerShuffle throws an IllegalArgumentException:

```
Shuffle ID [shuffleId] registered twice
```

registerShuffle is used when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#createShuffleMapStage[create a ShuffleMapStage] (for a xref:rdd:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and xref:scheduler:spark-scheduler-ActiveJob.adoc[ActiveJob]).

== [[registerMapOutputs]] Registering Map Outputs for Shuffle (Possibly with Epoch Change)

[source, scala]
----
registerMapOutputs(
  shuffleId: Int,
  statuses: Array[MapStatus],
  changeEpoch: Boolean = false): Unit
----

registerMapOutputs registers the input `statuses` (as the shuffle map output) with the input `shuffleId` in the xref:ROOT:MapOutputTracker.adoc#mapStatuses[mapStatuses] internal cache.

registerMapOutputs <<incrementEpoch, increments epoch>> if the input `changeEpoch` is enabled (it is not by default).

registerMapOutputs is used when `DAGScheduler` handles xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ShuffleMapTask[successful `ShuffleMapTask` completion] and xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[executor lost events].

== [[getSerializedMapOutputStatuses]] Finding Serialized Map Output Statuses (And Possibly Broadcasting Them)

[source, scala]
----
getSerializedMapOutputStatuses(
  shuffleId: Int): Array[Byte]
----

getSerializedMapOutputStatuses <<checkCachedStatuses, finds cached serialized map statuses>> for the input `shuffleId`.

If found, getSerializedMapOutputStatuses returns the cached serialized map statuses.

Otherwise, getSerializedMapOutputStatuses acquires the <<shuffleIdLocks, shuffle lock>> for `shuffleId` and <<checkCachedStatuses, finds cached serialized map statuses>> again since some other thread could not update the <<cachedSerializedStatuses, cachedSerializedStatuses>> internal cache.

getSerializedMapOutputStatuses returns the serialized map statuses if found.

If not, getSerializedMapOutputStatuses xref:ROOT:MapOutputTracker.adoc#serializeMapStatuses[serializes the local array of `MapStatuses`] (from <<checkCachedStatuses, checkCachedStatuses>>).

You should see the following INFO message in the logs:

```
Size of output statuses for shuffle [shuffleId] is [bytes] bytes
```

getSerializedMapOutputStatuses saves the serialized map output statuses in <<cachedSerializedStatuses, cachedSerializedStatuses>> internal cache if the <<epoch, epoch>> has not changed in the meantime. getSerializedMapOutputStatuses also saves its broadcast version in <<cachedSerializedBroadcast, cachedSerializedBroadcast>> internal cache.

If the <<epoch, epoch>> has changed in the meantime, the serialized map output statuses and their broadcast version are not saved, and you should see the following INFO message in the logs:

```
Epoch changed, not caching!
```

getSerializedMapOutputStatuses <<removeBroadcast, removes the broadcast>>.

getSerializedMapOutputStatuses returns the serialized map statuses.

getSerializedMapOutputStatuses is used when <<MessageLoop, MapOutputTrackerMaster responds to `GetMapOutputMessage` requests>> and xref:scheduler:DAGScheduler.adoc#createShuffleMapStage[`DAGScheduler` creates `ShuffleMapStage` for `ShuffleDependency`] (copying the shuffle map output locations from previous jobs to avoid unnecessarily regenerating data).

=== [[checkCachedStatuses]] Finding Cached Serialized Map Statuses

[source, scala]
----
checkCachedStatuses(): Boolean
----

checkCachedStatuses is an internal helper method that <<getSerializedMapOutputStatuses, getSerializedMapOutputStatuses>> uses to do some bookkeeping (when the <<epoch, epoch>> and <<cacheEpoch, cacheEpoch>> differ) and set local `statuses`, `retBytes` and `epochGotten` (that getSerializedMapOutputStatuses uses).

Internally, checkCachedStatuses acquires the xref:ROOT:MapOutputTracker.adoc#epochLock[`epochLock` lock] and checks the status of <<epoch, epoch>> to <<cacheEpoch, cached `cacheEpoch`>>.

If `epoch` is younger (i.e. greater), checkCachedStatuses clears <<cachedSerializedStatuses, cachedSerializedStatuses>> internal cache, <<clearCachedBroadcast, cached broadcasts>> and sets `cacheEpoch` to be `epoch`.

checkCachedStatuses gets the serialized map output statuses for the `shuffleId` (of the owning <<getSerializedMapOutputStatuses, getSerializedMapOutputStatuses>>).

When the serialized map output status is found, checkCachedStatuses saves it in a local `retBytes` and returns `true`.

When not found, you should see the following DEBUG message in the logs:

```
cached status not found for : [shuffleId]
```

checkCachedStatuses uses xref:ROOT:MapOutputTracker.adoc#mapStatuses[mapStatuses] internal cache to get map output statuses for the `shuffleId` (of the owning <<getSerializedMapOutputStatuses, getSerializedMapOutputStatuses>>) or falls back to an empty array and sets it to a local `statuses`. checkCachedStatuses sets the local `epochGotten` to the current <<epoch, epoch>> and returns `false`.

== [[PoisonPill]] PoisonPill Message

`PoisonPill` is a `GetMapOutputMessage` (with `-99` as `shuffleId`) that indicates that <<MessageLoop, MessageLoop>> should exit its message loop.

`PoisonPill` is posted when <<stop, MapOutputTrackerMaster stops>>.

== [[registerMapOutput]] Registering Shuffle Map Output

[source, scala]
----
registerMapOutput(
  shuffleId: Int,
  mapId: Int,
  status: MapStatus): Unit
----

registerMapOutput finds the xref:ROOT:ShuffleStatus.adoc[ShuffleStatus] by the given shuffle ID and xref:ROOT:ShuffleStatus.adoc#addMapOutput[adds the given MapStatus]:

* The given mapId is the xref:scheduler:Task.adoc#partitionId[partitionId] of the xref:scheduler:ShuffleMapTask.adoc[ShuffleMapTask] that finished.

* The given shuffleId is the xref:rdd:spark-rdd-ShuffleDependency.adoc#shuffleId[shuffleId] of the xref:rdd:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] of the xref:scheduler:spark-scheduler-ShuffleMapStage.adoc#shuffleDep[ShuffleMapStage] (for which the ShuffleMapTask completed)

registerMapOutput is used when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#handleTaskCompletion[handle a ShuffleMapTask completion].

== [[getStatistics]] `getStatistics` Method

[source, scala]
----
getStatistics(dep: ShuffleDependency[_, _, _]): MapOutputStatistics
----

`getStatistics`...FIXME

NOTE: `getStatistics` is used when...FIXME

== [[unregisterAllMapOutput]] unregisterAllMapOutput Method

[source, scala]
----
unregisterAllMapOutput(
  shuffleId: Int): Unit
----

unregisterAllMapOutput...FIXME

unregisterAllMapOutput is used when...FIXME

== [[unregisterShuffle]] unregisterShuffle Method

[source, scala]
----
unregisterShuffle(
  shuffleId: Int): Unit
----

unregisterShuffle...FIXME

unregisterShuffle is used when...FIXME

== [[removeOutputsOnHost]] removeOutputsOnHost Method

[source, scala]
----
removeOutputsOnHost(
  host: String): Unit
----

removeOutputsOnHost...FIXME

removeOutputsOnHost is used when...FIXME

== [[removeOutputsOnExecutor]] removeOutputsOnExecutor Method

[source, scala]
----
removeOutputsOnExecutor(
  execId: String): Unit
----

removeOutputsOnExecutor...FIXME

removeOutputsOnExecutor is used when...FIXME

== [[getNumAvailableOutputs]] getNumAvailableOutputs Method

[source, scala]
----
getNumAvailableOutputs(
  shuffleId: Int): Int
----

getNumAvailableOutputs...FIXME

getNumAvailableOutputs is used when...FIXME

== [[findMissingPartitions]] findMissingPartitions Method

[source, scala]
----
findMissingPartitions(
  shuffleId: Int): Option[Seq[Int]]
----

findMissingPartitions...FIXME

findMissingPartitions is used when...FIXME

== [[getMapSizesByExecutorId]] getMapSizesByExecutorId Method

[source, scala]
----
getMapSizesByExecutorId(
  shuffleId: Int,
  startPartition: Int,
  endPartition: Int): Iterator[(BlockManagerId, Seq[(BlockId, Long)])]
----

NOTE: getMapSizesByExecutorId is part of the xref:ROOT:MapOutputTracker.adoc#getMapSizesByExecutorId[MapOutputTracker] abstraction.

getMapSizesByExecutorId...FIXME

== [[logging]] Logging

Enable `ALL` logging level for `org.apache.spark.MapOutputTrackerMaster` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

[source]
----
log4j.logger.org.apache.spark.MapOutputTrackerMaster=ALL
----

Refer to xref:ROOT:spark-logging.adoc[Logging].

== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| [[cachedSerializedBroadcast]] `cachedSerializedBroadcast`
| Internal registry of...FIXME

Used when...FIXME

| [[cachedSerializedStatuses]] `cachedSerializedStatuses`
| Internal registry of serialized xref:scheduler:spark-scheduler-MapStatus.adoc[shuffle map output statuses] (as `Array[Byte]`) per...FIXME

Used when...FIXME

| [[cacheEpoch]] `cacheEpoch`
| Internal registry with...FIXME

Used when...FIXME

| [[shuffleIdLocks]] `shuffleIdLocks`
| Internal registry of locks for shuffle ids.

Used when...FIXME

|===
