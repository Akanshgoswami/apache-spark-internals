= FileCommitProtocol

`FileCommitProtocol` is an <<contract, abstraction>> of <<implementations, committers>> that can setup, commit or abort a job or task.

`FileCommitProtocol` is used for <<spark-rdd-PairRDDFunctions.adoc#saveAsNewAPIHadoopDataset, RDD.saveAsNewAPIHadoopDataset>> and <<spark-rdd-PairRDDFunctions.adoc#saveAsHadoopDataset, RDD.saveAsHadoopDataset>> transformations (that use `SparkHadoopWriter` utility to <<spark-internal-io-SparkHadoopWriter.adoc#write, write a key-value RDD out>>).

A concrete <<implementations, FileCommitProtocol>> is created using <<instantiate, FileCommitProtocol.instantiate>> utility.

[[contract]]
.FileCommitProtocol Contract (Abstract Methods Only)
[cols="30m,70",options="header",width="100%"]
|===
| Method
| Description

| abortJob
a| [[abortJob]]

[source, scala]
----
abortJob(
  jobContext: JobContext): Unit
----

| abortTask
a| [[abortTask]]

[source, scala]
----
abortTask(
  taskContext: TaskAttemptContext): Unit
----

| commitJob
a| [[commitJob]]

[source, scala]
----
commitJob(
  jobContext: JobContext,
  taskCommits: Seq[TaskCommitMessage]): Unit
----

| commitTask
a| [[commitTask]]

[source, scala]
----
commitTask(
  taskContext: TaskAttemptContext): TaskCommitMessage
----

| newTaskTempFile
a| [[newTaskTempFile]]

[source, scala]
----
newTaskTempFile(
  taskContext: TaskAttemptContext,
  dir: Option[String],
  ext: String): String
----

| newTaskTempFileAbsPath
a| [[newTaskTempFileAbsPath]]

[source, scala]
----
newTaskTempFileAbsPath(
  taskContext: TaskAttemptContext,
  absoluteDir: String,
  ext: String): String
----

| onTaskCommit
a| [[onTaskCommit]]

[source, scala]
----
onTaskCommit(
  taskCommit: TaskCommitMessage): Unit = {}
----

| setupJob
a| [[setupJob]]

[source, scala]
----
setupJob(
  jobContext: JobContext): Unit
----

| setupTask
a| [[setupTask]]

[source, scala]
----
setupTask(
  taskContext: TaskAttemptContext): Unit
----

Sets up the task with the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/TaskAttemptContext.html[TaskAttemptContext]

Used when `SparkHadoopWriter` is requested to <<spark-internal-io-SparkHadoopWriter.adoc#executeTask, executeTask>> (while <<spark-internal-io-SparkHadoopWriter.adoc#write, writing out a key-value RDD>>)

|===

[[implementations]]
.FileCommitProtocols
[cols="30,70",options="header",width="100%"]
|===
| FileCommitProtocol
| Description

| <<spark-internal-io-HadoopMapReduceCommitProtocol.adoc#, HadoopMapReduceCommitProtocol>>
| [[HadoopMapReduceCommitProtocol]]

| <<spark-internal-io-HadoopMapRedCommitProtocol.adoc#, HadoopMapRedCommitProtocol>>
| [[HadoopMapRedCommitProtocol]]

|===

== [[deleteWithJob]] `deleteWithJob` Method

[source, scala]
----
deleteWithJob(
  fs: FileSystem,
  path: Path,
  recursive: Boolean): Boolean
----

`deleteWithJob`...FIXME

NOTE: `deleteWithJob` is used when...FIXME

== [[instantiate]] Instantiating FileCommitProtocol -- `instantiate` Utility

[source, scala]
----
instantiate(
  className: String,
  jobId: String,
  outputPath: String,
  dynamicPartitionOverwrite: Boolean = false): FileCommitProtocol
----

`instantiate` prints out the following DEBUG message to the logs:

```
Creating committer [className]; job [jobId]; output=[outputPath]; dynamic=[dynamicPartitionOverwrite]
```

`instantiate` tries to find a constructor method that takes three arguments (two of type `String` and one `Boolean`) for the given `jobId`, `outputPath` and `dynamicPartitionOverwrite` flag.

`instantiate` prints out the following DEBUG message to the logs:

```
Using (String, String, Boolean) constructor
```

In case of `NoSuchMethodException`, `instantiate` prints out the following DEBUG message to the logs:

```
Falling back to (String, String) constructor
```

`instantiate` tries to find a constructor method that takes two arguments (two of type `String`) for the given `jobId` and `outputPath`.

With two `String` arguments, `instantiate` requires that the given `dynamicPartitionOverwrite` flag is disabled (`false`) or throws an `IllegalArgumentException`:

[options="wrap"]
----
requirement failed: Dynamic Partition Overwrite is enabled but the committer [className] does not have the appropriate constructor
----

NOTE: `instantiate` is used when <<spark-internal-io-HadoopMapRedWriteConfigUtil.adoc#createCommitter, HadoopMapRedWriteConfigUtil>> and <<spark-internal-io-HadoopMapReduceWriteConfigUtil.adoc#createCommitter, HadoopMapReduceWriteConfigUtil>> are requested to create a <<spark-internal-io-HadoopMapReduceCommitProtocol.adoc#, HadoopMapReduceCommitProtocol>>.
