= RpcEnv

RpcEnv is an abstraction of <<implementations, RPC systems>>.

== [[implementations]] Available RpcEnvs

xref:rpc:NettyRpcEnv.adoc[] is the default and only known RpcEnv in Apache Spark.

== [[creating-instance]] Creating Instance

RpcEnv takes the following to be created:

* [[conf]] xref:ROOT:SparkConf.adoc[]

RpcEnv is created using <<create, RpcEnv.create>> utility.

RpcEnv is an abstract class and cannot be created directly. It is created indirectly for the <<implementations, concrete RpcEnvs>>.

== [[create]] Creating RpcEnv

[source,scala]
----
create(
  name: String,
  host: String,
  port: Int,
  conf: SparkConf,
  securityManager: SecurityManager,
  clientMode: Boolean = false): RpcEnv // <1>
create(
  name: String,
  bindAddress: String,
  advertiseAddress: String,
  port: Int,
  conf: SparkConf,
  securityManager: SecurityManager,
  numUsableCores: Int,
  clientMode: Boolean): RpcEnv
----
<1> Uses 0 for numUsableCores

create creates a xref:rpc:NettyRpcEnvFactory.adoc[] and requests to xref:rpc:NettyRpcEnvFactory.adoc#create[create an RpcEnv] (with an xref:rpc:RpcEnvConfig.adoc[] with all the given arguments).

create is used when:

* SparkEnv utility is requested to xref:core:SparkEnv.adoc#create[create a SparkEnv] (clientMode flag is turned on for executors and off for the driver)

* With clientMode flag turned on:

** (Spark on YARN) ApplicationMaster is requested to xref:spark-on-yarn:spark-yarn-applicationmaster.adoc#runExecutorLauncher[runExecutorLauncher] (in client deploy mode with clientMode flag is turned on)

** ClientApp is requested to start

** (Spark Standalone) Master is requested to startRpcEnvAndEndpoint

** DriverWrapper standalone application is launched

** (Spark Standalone) Worker is requested to startRpcEnvAndEndpoint

** CoarseGrainedExecutorBackend is requested to xref:ROOT:spark-CoarseGrainedExecutorBackend.adoc#run[run]
