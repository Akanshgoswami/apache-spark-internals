= [[ShuffleMapStage]] ShuffleMapStage

*ShuffleMapStage* (_shuffle map stage_ or simply _map stage_) is one of the two types of xref:scheduler:Stage.adoc[stages] in a physical execution DAG (beside a xref:scheduler:spark-scheduler-ResultStage.adoc[ResultStage]).

ShuffleMapStage corresponds to (and is associated with) a <<shuffleDep, ShuffleDependency>>.

ShuffleMapStage is created when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#createShuffleMapStage[plan a ShuffleDependency for execution].

NOTE: ShuffleMapStage xref:scheduler:DAGScheduler.adoc#submitMapStage[can also be submitted independently as a Spark job] for xref:scheduler:DAGScheduler.adoc#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling].

NOTE: The *logical DAG* or *logical execution plan* is the xref:rdd:spark-rdd-lineage.adoc[RDD lineage].

[[isAvailable]]
When executed, a ShuffleMapStage saves *map output files* that can later be fetched by reduce tasks. When all map outputs are available, the ShuffleMapStage is considered *available* (or *ready*).

Output locations can be missing, i.e. partitions have not been calculated or are lost.

ShuffleMapStage uses <<outputLocs, outputLocs>> and <<_numAvailableOutputs, _numAvailableOutputs>> internal registries to track how many shuffle map outputs are available.

ShuffleMapStage is an input for the other following stages in the DAG of stages and is also called a *shuffle dependency's map side*.

A ShuffleMapStage may contain multiple *pipelined operations*, e.g. `map` and `filter`, before shuffle operation.

A single ShuffleMapStage <<stage-sharing, can be shared across different jobs>>.

== [[creating-instance]] Creating Instance

ShuffleMapStage takes the following to be created:

* [[id]] Stage ID
* [[rdd]] xref:rdd:spark-rdd-ShuffleDependency.adoc#rdd[RDD] of the <<shuffleDep, ShuffleDependency>>
* [[numTasks]] Number of tasks
* [[parents]] Parent xref:scheduler:Stage.adoc[stages]
* [[firstJobId]] ID of the xref:scheduler:spark-scheduler-ActiveJob.adoc[ActiveJob] that created it
* [[callSite]] CallSite
* [[shuffleDep]] xref:rdd:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]
* [[mapOutputTrackerMaster]] xref:ROOT:MapOutputTrackerMaster.adoc[MapOutputTrackerMaster]

ShuffleMapStage initializes the <<internal-registries, internal registries and counters>>.

== [[addOutputLoc]] Registering MapStatus For Partition

[source, scala]
----
addOutputLoc(partition: Int, status: MapStatus): Unit
----

`addOutputLoc` adds the input `status` to the <<outputLocs, output locations>> for the input `partition`.

`addOutputLoc` increments <<_numAvailableOutputs, `_numAvailableOutputs` internal counter>> if the input `MapStatus` is the first result for the `partition`.

NOTE: `addOutputLoc` is used when xref:scheduler:DAGScheduler.adoc#createShuffleMapStage[`DAGScheduler` creates a ShuffleMapStage for a `ShuffleDependency` and a `ActiveJob`] (and xref:ROOT:MapOutputTrackerMaster.adoc#containsShuffle[`MapOutputTrackerMaster` tracks some output locations of the `ShuffleDependency`]) and when xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ShuffleMapTask[`ShuffleMapTask` has finished].

== [[removeOutputLoc]] Removing MapStatus For Partition And BlockManager

[source, scala]
----
removeOutputLoc(partition: Int, bmAddress: BlockManagerId): Unit
----

`removeOutputLoc` removes the `MapStatus` for the input `partition` and `bmAddress` xref:ROOT:BlockManager.adoc[BlockManager] from the <<outputLocs, output locations>>.

`removeOutputLoc` decrements <<_numAvailableOutputs, `_numAvailableOutputs` internal counter>> if the the removed `MapStatus` was the last result for the `partition`.

NOTE: `removeOutputLoc` is exclusively used when a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[`Task` has failed with `FetchFailed` exception].

== [[findMissingPartitions]] Finding Missing Partitions

[source, scala]
----
findMissingPartitions(): Seq[Int]
----

NOTE: `findMissingPartitions` is part of xref:scheduler:Stage.adoc#contract[`Stage` contract] that returns the partitions that are missing, i.e. are yet to be computed.

Internally, `findMissingPartitions` uses <<outputLocs, `outputLocs` internal registry>> to find indices with empty lists of `MapStatus`.

== [[stage-sharing]] ShuffleMapStage Sharing

A ShuffleMapStage can be shared across multiple jobs, if these jobs reuse the same RDDs.

When a ShuffleMapStage is submitted to DAGScheduler to execute, `getShuffleMapStage` is called.

[source, scala]
----
scala> val rdd = sc.parallelize(0 to 5).map((_,1)).sortByKey()  // <1>

scala> rdd.count  // <2>

scala> rdd.count  // <3>
----
<1> Shuffle at `sortByKey()`
<2> Submits a job with two stages with two being executed
<3> Intentionally repeat the last action that submits a new job with two stages with one being shared as already-being-computed

.Skipped Stages are already-computed ShuffleMapStages
image::dagscheduler-webui-skipped-stages.png[align="center"]

== [[numAvailableOutputs]] Number of Partitions with Shuffle Map Outputs Available

[source, scala]
----
numAvailableOutputs: Int
----

`numAvailableOutputs` returns <<_numAvailableOutputs, _numAvailableOutputs>> internal registry.

NOTE: `numAvailableOutputs` is used exclusively when xref:scheduler:DAGScheduler.adoc#submitMissingTasks[`DAGScheduler` submits missing tasks for ShuffleMapStage] (and only to print a DEBUG message when the ShuffleMapStage is finished).

== [[mapStageJobs]] Returning Collection of Active Jobs

[source, scala]
----
mapStageJobs: Seq[ActiveJob]
----

`mapStageJobs` returns <<_mapStageJobs, _mapStageJobs>> internal registry.

NOTE: `mapStageJobs` is used exclusively when xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[`DAGScheduler` is notified that a `ShuffleMapTask` has finished successfully] (and the task made ShuffleMapStage completed and so marks any map-stage jobs waiting on this stage as finished).

== [[addActiveJob]] Registering Job (that Computes ShuffleDependency)

[source, scala]
----
addActiveJob(job: ActiveJob): Unit
----

`addActiveJob` registers the input link:spark-scheduler-ActiveJob.adoc[ActiveJob] in <<_mapStageJobs, _mapStageJobs>> internal registry.

NOTE: The `ActiveJob` is added as the first element in `_mapStageJobs`.

NOTE: `addActiveJob` is used exclusively when xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[`DAGScheduler` is notified that a `ShuffleDependency` was submitted] (and so a new `ActiveJob` is created to compute it).

== [[removeActiveJob]] Deregistering Job

[source, scala]
----
removeActiveJob(job: ActiveJob): Unit
----

`removeActiveJob` removes a `ActiveJob` from <<_mapStageJobs, _mapStageJobs>> internal registry.

NOTE: `removeActiveJob` is used exclusively when xref:scheduler:DAGScheduler.adoc#cleanupStateForJobAndIndependentStages[`DAGScheduler` cleans up after `ActiveJob` has finished] (regardless of the outcome).

== [[removeOutputsOnExecutor]] Removing All Shuffle Outputs Registered for Lost Executor

[source, scala]
----
removeOutputsOnExecutor(execId: String): Unit
----

`removeOutputsOnExecutor` removes all `MapStatuses` with the input `execId` executor from the <<outputLocs, outputLocs>> internal registry (of `MapStatuses` per partition).

If the input `execId` had the last registered `MapStatus` for a partition, `removeOutputsOnExecutor` decrements <<_numAvailableOutputs, _numAvailableOutputs>> counter and you should see the following INFO message in the logs:

```
INFO [stage] is now unavailable on executor [execId] ([_numAvailableOutputs]/[numPartitions], [isAvailable])
```

NOTE: `removeOutputsOnExecutor` is used exclusively when xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[`DAGScheduler` cleans up after a lost executor].

== [[outputLocInMapOutputTrackerFormat]] Preparing Shuffle Map Outputs in MapOutputTracker Format

[source, scala]
----
outputLocInMapOutputTrackerFormat(): Array[MapStatus]
----

`outputLocInMapOutputTrackerFormat` returns the first (if available) element for every partition from <<outputLocs, outputLocs>> internal registry. If there is no entry for a partition, that position is filled with `null`.

[NOTE]
====
`outputLocInMapOutputTrackerFormat` is used when `DAGScheduler` is xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[notified that a `ShuffleMapTask` has finished successfully] (and the corresponding ShuffleMapStage is complete) and xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[cleans up after a lost executor].

In both cases, `outputLocInMapOutputTrackerFormat` is used to xref:ROOT:MapOutputTrackerMaster.adoc#registerMapOutputs[register the shuffle map outputs (of the `ShuffleDependency`) with `MapOutputTrackerMaster`].
====

== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| [[_mapStageJobs]] `_mapStageJobs`
| link:spark-scheduler-ActiveJob.adoc[ActiveJobs] associated with the ShuffleMapStage.

A new `ActiveJob` can be <<addActiveJob, registered>> and <<removeActiveJob, deregistered>>.

The list of `ActiveJobs` registered are available using <<mapStageJobs, mapStageJobs>>.

| [[outputLocs]] `outputLocs`
| Tracks xref:scheduler:MapStatus.adoc[MapStatuses] for each partition.

There could be many `MapStatus` entries per partition due to link:spark-taskschedulerimpl-speculative-execution.adoc[Speculative Execution of Tasks].

When <<creating-instance, ShuffleMapStage is created>>, `outputLocs` is empty, i.e. all elements are empty lists.

The size of `outputLocs` is exactly the number of partitions of the xref:scheduler:Stage.adoc#rdd[RDD the stage runs on].

| [[_numAvailableOutputs]] `_numAvailableOutputs`
| The number of available outputs for the partitions of the ShuffleMapStage.

`_numAvailableOutputs` is incremented when the <<addOutputLoc, first `MapStatus` is registered for a partition>> (that could be more tasks per partition) and decrements when the <<removeOutputLoc, last `MapStatus` is removed for a partition>>.

`_numAvailableOutputs` should not be greater than the number of partitions (and hence the number of `MapStatus` collections in <<outputLocs, outputLocs>> internal registry).

|===
