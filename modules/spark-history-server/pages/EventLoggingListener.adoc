= [[EventLoggingListener]] EventLoggingListener -- Spark Listener For Persisting Events

`EventLoggingListener` is a xref:ROOT:spark-scheduler-SparkListener.adoc[SparkListener] that <<logEvent, persists JSON-encoded events>> of a Spark application with event logging enabled (based on xref:configuration-properties.adoc#spark.eventLog.enabled[spark.eventLog.enabled] configuration property).

`EventLoggingListener` writes out log files to a directory (based on xref:configuration-properties.adoc#spark.eventLog.dir[spark.eventLog.dir] configuration property). All xref:ROOT:spark-scheduler-SparkListener.adoc[Spark events] are logged (but xref:ROOT:spark-scheduler-SparkListener.adoc#SparkListenerBlockUpdated[SparkListenerBlockUpdated] and xref:ROOT:spark-scheduler-SparkListener.adoc#SparkListenerExecutorMetricsUpdate[SparkListenerExecutorMetricsUpdate]).

TIP: Use xref:index.adoc[Spark History Server] to view the event logs in a browser (similarly to xref:webui:index.adoc[web UI] of a Spark application).

[[inprogress-extension]]
`EventLoggingListener` uses *.inprogress* file extension for in-flight event log files of active Spark applications.

`EventLoggingListener` can compress events (based on xref:configuration-properties.adoc#spark.eventLog.compress[spark.eventLog.compress] configuration property).

[[logging]]
[TIP]
====
Enable `ALL` logging level for `org.apache.spark.scheduler.EventLoggingListener` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.scheduler.EventLoggingListener=ALL
```

Refer to xref:ROOT:spark-logging.adoc[Logging].
====

== [[creating-instance]] Creating EventLoggingListener Instance

`EventLoggingListener` takes the following to be created:

* [[appId]] Application ID
* [[appAttemptId]] Application Attempt ID (optional)
* [[logBaseDir]] Log Directory
* [[sparkConf]] xref:ROOT:spark-SparkConf.adoc[SparkConf]
* [[hadoopConf]] Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration]

`EventLoggingListener` initializes the <<internal-properties, internal properties>>.

NOTE: When initialized with no <<hadoopConf, Hadoop Configuration>>, `EventLoggingListener` uses `SparkHadoopUtil` utility to xref:ROOT:spark-SparkHadoopUtil.adoc#newConfiguration[create a new one].

== [[start]] Starting EventLoggingListener -- `start` Method

[source, scala]
----
start(): Unit
----

`start` checks whether `logBaseDir` is really a directory, and if it is not, it throws a `IllegalArgumentException` with the following message:

```
Log directory [logBaseDir] does not exist.
```

The log file's working name is created based on `appId` with or without the compression codec used and `appAttemptId`, i.e. `local-1461696754069`. It also uses `.inprogress` extension.

If <<spark_eventLog_overwrite, overwrite is enabled>>, you should see the WARN message:

```
Event log [path] already exists. Overwriting...
```

The working log `.inprogress` is attempted to be deleted. In case it could not be deleted, the following WARN message is printed out to the logs:

```
Error deleting [path]
```

The buffered output stream is created with metadata with Spark's version and `SparkListenerLogStart` class' name as the first line.

```
{"Event":"SparkListenerLogStart","Spark Version":"2.0.0-SNAPSHOT"}
```

At this point, `EventLoggingListener` is ready for event logging and you should see the following INFO message in the logs:

```
Logging events to [logPath]
```

NOTE: `start` is executed while `SparkContext` is xref:ROOT:spark-SparkContext-creating-instance-internals.adoc#_eventLogger[created].

== [[logEvent]] Logging Event as JSON -- `logEvent` Method

[source, scala]
----
logEvent(
  event: SparkListenerEvent,
  flushLogger: Boolean = false): Unit
----

`logEvent` logs the given `event` as JSON.

CAUTION: FIXME

== [[stop]] Stopping EventLoggingListener -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` closes `PrintWriter` for the log file and renames the file to be without `.inprogress` extension.

If the target log file exists (one without `.inprogress` extension), it overwrites the file if <<spark_eventLog_overwrite, spark.eventLog.overwrite>> is enabled. You should see the following WARN message in the logs:

```
Event log [target] already exists. Overwriting...
```

If the target log file exists and overwrite is disabled, an `java.io.IOException` is thrown with the following message:

```
Target log file already exists ([logPath])
```

NOTE: `stop` is executed while `SparkContext` is requested to xref:ROOT:spark-SparkContext.adoc#stop[stop].
