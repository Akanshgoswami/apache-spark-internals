= SortShuffleManager

*SortShuffleManager* is the default and only known xref:ShuffleManager.adoc[ShuffleManager] in Apache Spark (with the short name `sort` or `tungsten-sort`).

NOTE: Use xref:ROOT:spark-configuration-properties.adoc#spark.shuffle.manager[spark.shuffle.manager] configuration property to activate custom xref:ShuffleManager.adoc[ShuffleManager].

SortShuffleManager is <<creating-instance, created>> when `SparkEnv` is xref:ROOT:spark-SparkEnv.adoc#ShuffleManager[created] (on the driver and executors at the very beginning of a Spark application's lifecycle).

.SortShuffleManager and SparkEnv (Driver and Executors)
image::SortShuffleManager.png[align="center"]

[[MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE]]
SortShuffleManager allows for `(1 << 24)` partition identifiers that can be encoded (i.e. `16777216`).

== [[creating-instance]] Creating Instance

[[conf]]
SortShuffleManager takes a single xref:ROOT:spark-SparkConf.adoc[SparkConf] to be created.

SortShuffleManager initializes the <<internal-properties, internal properties>>.

== [[shuffleBlockResolver]] IndexShuffleBlockResolver

[source, scala]
----
shuffleBlockResolver: ShuffleBlockResolver
----

NOTE: `shuffleBlockResolver` is part of the <<ShuffleManager.adoc#shuffleBlockResolver, ShuffleManager>> abstraction.

`shuffleBlockResolver` is an xref:spark-shuffle-IndexShuffleBlockResolver.adoc[IndexShuffleBlockResolver] that is created immediately when SortShuffleManager is.

`shuffleBlockResolver` is used when SortShuffleManager is requested for a <<getWriter, ShuffleWriter for a given partition>>, to <<unregisterShuffle, unregister a shuffle metadata>> and <<stop, stop>>.

== [[getWriter]] Getting ShuffleWriter For ShuffleHandle -- `getWriter` Method

[source, scala]
----
getWriter[K, V](
  handle: ShuffleHandle,
  mapId: Int,
  context: TaskContext): ShuffleWriter[K, V]
----

NOTE: `getWriter` is part of xref:ShuffleManager.adoc#getWriter[ShuffleManager] contract to give a xref:spark-shuffle-ShuffleWriter.adoc[ShuffleWriter] for a given xref:spark-shuffle-ShuffleHandle.adoc[ShuffleHandle].

Internally, `getWriter` registers the given ShuffleHandle (by the xref:spark-shuffle-ShuffleHandle.adoc#shuffleId[shuffleId] and xref:spark-shuffle-BaseShuffleHandle.adoc#numMaps[numMaps]) in the <<numMapsForShuffle, numMapsForShuffle>> internal registry unless already done.

NOTE: getWriter expects that the input ShuffleHandle is of type xref:spark-shuffle-BaseShuffleHandle.adoc[BaseShuffleHandle]. Moreover, getWriter further expects that in two (out of three cases) it is a more specialized xref:spark-shuffle-IndexShuffleBlockResolver.adoc[IndexShuffleBlockResolver].

`getWriter` then creates a new ShuffleWriter based on the type of the given ShuffleHandle.

[cols="2",options="header",width="100%"]
|===
| ShuffleHandle
| ShuffleWriter

| xref:spark-shuffle-SerializedShuffleHandle.adoc[SerializedShuffleHandle]
| xref:spark-shuffle-UnsafeShuffleWriter.adoc[UnsafeShuffleWriter]

| xref:spark-shuffle-BypassMergeSortShuffleHandle.adoc[BypassMergeSortShuffleHandle]
| xref:spark-shuffle-BypassMergeSortShuffleWriter.adoc[BypassMergeSortShuffleWriter]

| xref:spark-shuffle-BaseShuffleHandle.adoc[BaseShuffleHandle]
| xref:spark-shuffle-SortShuffleWriter.adoc[SortShuffleWriter]

|===

== [[unregisterShuffle]] Unregistering Shuffle -- `unregisterShuffle` Method

[source, scala]
----
unregisterShuffle(shuffleId: Int): Boolean
----

NOTE: `unregisterShuffle` is part of the xref:ShuffleManager.adoc#unregisterShuffle[ShuffleManager] contract.

`unregisterShuffle` tries to remove the given `shuffleId` from the <<numMapsForShuffle, numMapsForShuffle>> internal registry.

If the given `shuffleId` was registered, `unregisterShuffle` requests the <<shuffleBlockResolver, IndexShuffleBlockResolver>> to <<spark-shuffle-IndexShuffleBlockResolver.adoc#removeDataByMap, remove the shuffle index and data files>> one by one (up to the number of mappers producing the output for the shuffle).

== [[registerShuffle]] Creating ShuffleHandle (For ShuffleDependency) -- `registerShuffle` Method

[source, scala]
----
registerShuffle[K, V, C](
  shuffleId: Int,
  numMaps: Int,
  dependency: ShuffleDependency[K, V, C]): ShuffleHandle
----

NOTE: `registerShuffle` is part of xref:ShuffleManager.adoc#registerShuffle[ShuffleManager] contract.

CAUTION: FIXME Copy the conditions

`registerShuffle` returns a new `ShuffleHandle` that can be one of the following:

1. link:spark-shuffle-BypassMergeSortShuffleHandle.adoc[BypassMergeSortShuffleHandle] (with `ShuffleDependency[K, V, V]`) when <<shouldBypassMergeSort, `shouldBypassMergeSort` condition holds>>.

2. link:spark-shuffle-SerializedShuffleHandle.adoc[SerializedShuffleHandle] (with `ShuffleDependency[K, V, V]`) when <<canUseSerializedShuffle, `canUseSerializedShuffle` condition holds>>.

3. link:spark-shuffle-BaseShuffleHandle.adoc[BaseShuffleHandle]

== [[getReader]] Creating BlockStoreShuffleReader For ShuffleHandle And Reduce Partitions -- `getReader` Method

[source, scala]
----
getReader[K, C](
  handle: ShuffleHandle,
  startPartition: Int,
  endPartition: Int,
  context: TaskContext): ShuffleReader[K, C]
----

NOTE: `getReader` is part of xref:ShuffleManager.adoc#getReader[ShuffleManager] contract.

`getReader` returns a new link:spark-shuffle-BlockStoreShuffleReader.adoc[BlockStoreShuffleReader] passing all the input parameters on to it.

NOTE: `getReader` assumes that the input `ShuffleHandle` is of type link:spark-shuffle-BaseShuffleHandle.adoc[BaseShuffleHandle].

== [[stop]] Stopping SortShuffleManager -- `stop` Method

[source, scala]
----
stop(): Unit
----

NOTE: `stop` is part of xref:ShuffleManager.adoc#stop[ShuffleManager contract] to stop the shuffle system.

`stop` simply requests the <<shuffleBlockResolver, IndexShuffleBlockResolver>> to <<spark-shuffle-IndexShuffleBlockResolver.adoc#stop, stop>> (which actually does nothing).

== [[shouldBypassMergeSort]] Considering BypassMergeSortShuffleHandle for ShuffleHandle -- `shouldBypassMergeSort` Method

[source, scala]
----
shouldBypassMergeSort(conf: SparkConf, dep: ShuffleDependency[_, _, _]): Boolean
----

`shouldBypassMergeSort` holds (i.e. is positive) when:

1. The input xref:rdd:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] has xref:rdd:spark-rdd-ShuffleDependency.adoc#mapSideCombine[`mapSideCombine` flag enabled] and xref:rdd:spark-rdd-ShuffleDependency.adoc#aggregator[`aggregator` defined].

2. `mapSideCombine` flag is disabled (i.e. `false`) but the xref:rdd:spark-rdd-ShuffleDependency.adoc#partitioner[number of partitions (of the `Partitioner` of the input `ShuffleDependency`)] is at most <<spark_shuffle_sort_bypassMergeThreshold, spark.shuffle.sort.bypassMergeThreshold>> Spark property (which defaults to `200`).

Otherwise, `shouldBypassMergeSort` does not hold (i.e. `false`).

NOTE: `shouldBypassMergeSort` is exclusively used when <<registerShuffle, SortShuffleManager selects a `ShuffleHandle`>> (for a `ShuffleDependency`).

== [[canUseSerializedShuffle]] Considering SerializedShuffleHandle for ShuffleHandle -- `canUseSerializedShuffle` Method

[source, scala]
----
canUseSerializedShuffle(dependency: ShuffleDependency[_, _, _]): Boolean
----

`canUseSerializedShuffle` condition holds (i.e. is positive) when all of the following hold (checked in that order):

1. The xref:ROOT:spark-Serializer.adoc#supportsRelocationOfSerializedObjects[`Serializer` of the input `ShuffleDependency` supports relocation of serialized objects].

2. The xref:rdd:spark-rdd-ShuffleDependency.adoc#aggregator[`Aggregator` of the input `ShuffleDependency` is _not_ defined].

3. The xref:rdd:spark-rdd-ShuffleDependency.adoc#partitioner[number of shuffle output partitions of the input `ShuffleDependency`] is at most the supported maximum number (which is `(1 << 24) - 1`, i.e. `16777215`).

You should see the following DEBUG message in the logs when `canUseSerializedShuffle` holds:

```
DEBUG Can use serialized shuffle for shuffle [id]
```

Otherwise, `canUseSerializedShuffle` does not hold and you should see one of the following DEBUG messages:

```
DEBUG Can't use serialized shuffle for shuffle [id] because the serializer, [name], does not support object relocation

DEBUG SortShuffleManager: Can't use serialized shuffle for shuffle [id] because an aggregator is defined

DEBUG Can't use serialized shuffle for shuffle [id] because it has more than [number] partitions
```

NOTE: `canUseSerializedShuffle` is exclusively used when <<registerShuffle, SortShuffleManager selects a `ShuffleHandle`>> (for a `ShuffleDependency`).

== [[logging]] Logging

Enable `ALL` logging level for `org.apache.spark.shuffle.sort.SortShuffleManager$` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

[source]
----
log4j.logger.org.apache.spark.shuffle.sort.SortShuffleManager$=ALL
----

Refer to xref:ROOT:spark-logging.adoc[Logging].

== [[internal-properties]] Internal Properties

[cols="30m,70",options="header",width="100%"]
|===
| Name
| Description

| numMapsForShuffle
| [[numMapsForShuffle]] Lookup table with the number of mappers producing the output for a shuffle (as {java-javadoc-url}/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap])

|===
