= [[ExternalSorter]] ExternalSorter

*ExternalSorter* is a `Spillable` of `WritablePartitionedPairCollection` of pairs (of K keys and C values).

`ExternalSorter[K, V, C]` is a parameterized type of `K` keys, `V` values, and `C` combiner (partial) values.

== [[creating-instance]] Creating Instance

ExternalSorter takes the following to be created:

* [[context]] xref:scheduler:spark-TaskContext.adoc[TaskContext]
* [[aggregator]] Optional xref:rdd:spark-Aggregator.adoc[Aggregator] (default: undefined)
* [[partitioner]] Optional xref:rdd:Partitioner[Partitioner] (default: undefined)
* [[ordering]] Optional Scala's http://www.scala-lang.org/api/current/scala/math/Ordering.html[Ordering] for keys (default: undefined)
* [[serializer]] xref:ROOT:spark-Serializer.adoc[Serializer] (default: xref:ROOT:spark-SparkEnv.adoc#serializer[system Serializer])

ExternalSorter is created when:

* SortShuffleWriter is requested to xref:shuffle:SortShuffleWriter.adoc#write[write records] (as a `ExternalSorter[K, V, C]` or `ExternalSorter[K, V, V]` based on xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag])

* BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read records] (with sort ordering defined)

== [[spills]] Spills Internal Registry

ExternalSorter manages spilled files.

== [[insertAll]] Inserting Records

[source, scala]
----
insertAll(
  records: Iterator[Product2[K, V]]): Unit
----

insertAll...FIXME

insertAll is used when:

* SortShuffleWriter is requested to xref:shuffle:SortShuffleWriter.adoc#write[write records] (as a `ExternalSorter[K, V, C]` or `ExternalSorter[K, V, V]` based on xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag])

* BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read records] (with sort ordering defined)

== [[writePartitionedFile]] Writing Partitioned File

[source, scala]
----
writePartitionedFile(
  blockId: BlockId,
  outputFile: File): Array[Long]
----

writePartitionedFile...FIXME

writePartitionedFile is used when SortShuffleWriter is requested to xref:shuffle:SortShuffleWriter.adoc#write[write records].

== [[stop]] Stopping ExternalSorter

[source, scala]
----
stop(): Unit
----

stop...FIXME

stop is used when:

* BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read records] (with sort ordering defined)

* SortShuffleWriter is requested to xref:shuffle:SortShuffleWriter.adoc#stop[stop]

== [[spill]] Spilling Data to Disk

[source, scala]
----
spill(
  collection: WritablePartitionedPairCollection[K, C]): Unit
----

spill requests the given WritablePartitionedPairCollection for a destructive WritablePartitionedIterator.

spill <<spillMemoryIteratorToDisk, spillMemoryIteratorToDisk>> (with the destructive WritablePartitionedIterator) that creates a SpilledFile.

spill adds the SpilledFile to the <<spills, spills>> internal registry.

spill is part of the Spillable abstraction.

== [[spillMemoryIteratorToDisk]] spillMemoryIteratorToDisk Internal Method

[source, scala]
----
spillMemoryIteratorToDisk(
  inMemoryIterator: WritablePartitionedIterator): SpilledFile
----

spillMemoryIteratorToDisk...FIXME

spillMemoryIteratorToDisk is used when:

* ExternalSorter is requested to <<spill, spill>>

* SpillableIterator is requested to spill

== [[maybeSpillCollection]] Spilling In-Memory Collection to Disk

[source, scala]
----
maybeSpillCollection(
  usingMap: Boolean): Unit
----

maybeSpillCollection...FIXME

maybeSpillCollection is used when ExternalSorter is requested to <<insertAll, insertAll>>.

== [[logging]] Logging

Enable `ALL` logging level for `org.apache.spark.util.collection.ExternalSorter` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

[source]
----
log4j.logger.org.apache.spark.util.collection.ExternalSorter=ALL
----

Refer to xref:ROOT:spark-logging.adoc[Logging].
